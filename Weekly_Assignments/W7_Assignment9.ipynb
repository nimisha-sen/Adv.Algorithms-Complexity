{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 9 - Markov Chain \n",
    "Implement a Markov chain working with whole words instead of letters.\n",
    "Leave the number of words in a sequence as an adjustable parameter, but due to memory limitations keep it at two.\n",
    "\n",
    "Example: k=2 “word1 word2” as one of the possible sequences.\n",
    "\n",
    "To test your implementation, you can use a smaller text.\n",
    "If you want, you can try to use something larger as an input text (a small book).\n",
    "\n",
    "Hint: Use a sparse dictionary from scipy.sparse.dok_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with an input text whose phrases will be used to generate the output. For every pair of words in the input, record a list of the possible words that come after the word pair.\n",
    "2. Once the datastructure is built, we can generate as much or as little output as we want. Start with any pair of words that occurs in the input, and then randomly pick one of the possible third words. Then move along, so the second word in the pair is the word you just generated, randomly pick another word, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/nimishasen/anaconda3/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in /Users/nimishasen/anaconda3/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/nimishasen/anaconda3/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.10.3-cp310-cp310-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nimishasen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melancholy', 'expression', '.', 'gregor', 'needed', 'to', 'make', ',', 'threw', 'himself', 'with', 'this', 'meant', 'not', 'something', 'more', '.', '“', 'what', 'is', 'posted', 'with', 'his', 'mother', 'would', 'wake', 'him', 'that', 'she', 'said', 'gregor', 'turned', 'around', 'gregor', '’', 'm', 'sure', 'that', 'they', 'might', 'go', 'silent', ',', 'please', 'follow', 'the', 'ground', 'so', 'much', 'food']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Preprocess the text by tokenizing and converting to lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def create_markov_chain(states, text):\n",
    "    transition_matrix = defaultdict(dict)\n",
    "\n",
    "    for i in range(len(text) - 1):\n",
    "        current_state = text[i]\n",
    "        next_state = text[i + 1]\n",
    "\n",
    "        if next_state not in transition_matrix[current_state]:\n",
    "            transition_matrix[current_state][next_state] = 1\n",
    "        else:\n",
    "            transition_matrix[current_state][next_state] += 1\n",
    "\n",
    "    # Normalize transition probabilities\n",
    "    for state in transition_matrix:\n",
    "        total_transitions = sum(transition_matrix[state].values())\n",
    "        for next_state in transition_matrix[state]:\n",
    "            transition_matrix[state][next_state] /= total_transitions\n",
    "\n",
    "    return lambda num_words, seed_word: simulate_chain(num_words, seed_word, transition_matrix)\n",
    "\n",
    "def simulate_chain(num_words, seed_word, transition_matrix):\n",
    "    current_word = seed_word\n",
    "    generated_sequence = [current_word]\n",
    "\n",
    "    for _ in range(num_words - 1):\n",
    "        if current_word in transition_matrix:\n",
    "            next_word = random.choices(\n",
    "                list(transition_matrix[current_word].keys()),\n",
    "                weights=list(transition_matrix[current_word].values())\n",
    "            )[0]\n",
    "            generated_sequence.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return generated_sequence\n",
    "\n",
    "def generate_markov_chain_from_file(file_path, k=2, num_words=20, seed_word=None):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    states = list(set(preprocessed_text))\n",
    "    markov_chain = create_markov_chain(states, preprocessed_text)\n",
    "\n",
    "    if seed_word is None:\n",
    "        seed_word = random.choice(states)\n",
    "\n",
    "    generated_sequence = markov_chain(num_words=num_words, seed_word=seed_word)\n",
    "    return generated_sequence\n",
    "\n",
    "file_path = 'Metamorphosis_FKafka.txt'\n",
    "\n",
    "# Generate Markov chain from the text file\n",
    "generated_sequence = generate_markov_chain_from_file(file_path, num_words=50, seed_word='melancholy')\n",
    "print(generated_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
